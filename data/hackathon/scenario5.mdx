---
title: Single Hybrid Cloud Platform for AI and Intelligent Apps
exercise: 5
date: '2024-06-21'
tags: ['openshift','ai','kubernetes']
draft: false
authors: ['default']
summary: "Let's show integration between AI and Intelligent Application workloads, and the effieciencies acheived by using one platform for both"
---

The ACME Financial Services team are underway with the OpenShift AI Proof of Concept and are now in position to integrate the AI model with a client application that uses the model for inference.
The ACME team is using more traditional `Predictive AI` use case that uses an object detection AI model to determine what object appears inside a webpage that uses a webcam to detect certain objects.

Their Data Science team have already deployed such a model, in a similar fashion to the model server you deployed earlier.

You challenge is to integrate their web application with the AI model. You will need to a) deploy it, b) configure it to talk to the model server.

## 5.1 - Deploy Application
Your first task to deploy the web application inside a new OpenShift project to house this web app.

## 5.1.1 - Deploy Application helper
Prior to your challenge, there is a helper deployment, that you will require. The first thing you will need to do is deply that. Here it is:
https://raw.githubusercontent.com/rh-aiservices-bu/mad_m6_workshop/main/deployment/pre_post_processor_deployment.yaml

## 5.1.2 - Deploy the application.
Here are some data point you will require
- the container image for the web app is here: quay.io/rh-aiservices-bu/mad-m6-workshop-intelligent-application:latest
- These Environment variables should be set:
 - OBJECT_DETECTION_URL: The model server host with path `/predictions` appended
 - DISPLAY_BOX: true


Documentation you may find helpful is:
- https://docs.redhat.com/en/documentation/openshift_container_platform/4.15/html/building_applications/creating-applications#odc-deploying-container-image_odc-creating-applications-using-developer-perspective
- https://docs.openshift.com/container-platform/3.11/dev_guide/environment_variables.html


## 5.2 - Test Application
Once you have deployed your application, open its Route in the Developer > Topology view.
You will need to allow webcam access to the application. Go ahead and take a photo. It should detect any of these:
- bottles
- caps/hats
- tshirts
The app should draw a bounding box around any of these - or indeed of anything you placed in front of the webcam and snapped a shot.


## 5.3 - Check your work
If your ACME Financial Services web app has successfully made an inference call to the Object Detection AI model server, it should have drawn a bounding box on the screen.
Please post a screenshot of it to  #event-anz-ai-hackathon with the message:

Please review [team name] solution for exercise 5.

This exercise is worth 25 points. The event team will reply in slack to confirm your updated team total score.



## 1.5 - Hints!
The first hint is free: In scenario 6, you will need to provision 15 minutes time for synthetic data generation as well as 20 minutes for model training. You might want to make this part of your strategy to win.

If you get stuck on a question, fear not, perhaps try a different approach. If you have tried everything you can think of and are still stuck you can unlock a hint for `5` points by posting a message in the `#event-anz-ocp-ai-hackathon` channel with the message:

> [team name] are stuck on [exercise] and are unlocking a hint.

A hackathon organiser will join your breakout room to share the hint with you ðŸ¤«.


TODO Tom - move this to a Google Doc
# HINTS
- [5.1.2]: The actual web app yaml (already with the configuration to talk to the model server) is available here: https://raw.githubusercontent.com/rh-aiservices-bu/mad_m6_workshop/main/deployment/intelligent_application_deployment.yaml

